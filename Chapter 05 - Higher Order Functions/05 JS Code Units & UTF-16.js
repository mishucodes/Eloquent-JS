//JS uses 16-bits to represent a character. It is based on Unicode. 16 bits are enough for most characters, but for those that go beyond 16-bits (Later Added Scripts & Emojis), it uses a pair of 02 such units.

//SOME NOTES ON PROBLEMS WITH UTF-16:
    //a) There is now a difference b/w "Code Units" & "Character Units". Character Units represent actual Unicode characters, & the other represent the actual 16-bits in the memory.
    //b) Sadly, operations like str.length use "Code Units", which is why emojis are twice as expensive on Twitter.
            console.log('ğŸ˜„â¤ï¸'.length);
            console.log('hello'[0]); //works...
            console.log('ğŸ˜„â¤ï¸'[0]); //something weird...
    //c) We can use these methods to get "Code Units" & "Character Units":
            console.log('ğŸ˜„'.charCodeAt(0)); //data in the first 16-bits (55357)...
            console.log('ğŸ˜„'.codePointAt(0)); //actual Unicode value of this emoji (128516)...
            console.log('â¤ï¸'.charCodeAt(0)); //but notice this one (10084)...
            console.log('â¤ï¸'.codePointAt(0)); //its value is exactly the same as above's (10084). Here's why:
        //ca) 'â¤ï¸' is actually NOT a single emoji code point. It's a sequence of two Unicode characters:
            //"U+2764" stands for "HEAVY BLACK HEART" -> â¤
            //"U+FE0F" stands for  "Variation Selector-16" (it forces emoji-style rendering).
            //Since â¤ (U+2764) fits within 16-bits, both charCodeAt(0) and codePointAt(0) return the same number (10084).
            //For emojis that require surrogate pairs (like ğŸ˜„), charCodeAt only shows the first half of the pair, but codePointAt combines both halves to give the real Unicode value.
            //Also, this first half's (High Surrogate) job is to signal to the computer that: (i): it's a non-16-bit character, (ii) this is the class it belongs to. The latter half (Low Surrogate) fills in more details. For example:
                console.log('ğŸ˜„'.charCodeAt(0)); //data in the first 16-bits (55357)...
                console.log('ğŸ˜‡'.charCodeAt(0)); //same as above (55357)...
                console.log('ğŸ¥¸'.charCodeAt(0)); //a little different (55358)...
            //For more information, try studying Chapter 13 from "Code" authored by Charles Petzold.
            //A COMPLAINT & CAUTION: I think it's a little immature of JS to give me an 'charPoint' when I do:
                console.log('ğŸ˜€'.codePointAt(0)); //this is good...
                console.log('ğŸ˜€'.codePointAt(1)); //this is bad...
                console.log('ğŸ˜€'.charCodeAt(1));  //this is for proof's sake...
    //d) IMPORTANT: When we run a for-of loop on a string, it loops over characters not 16-bit units. For example:
            let str = 'ğŸ˜„â¤ï¸aWeirdSpaceAboveMe'; //with some devil in the details, ofc.
            for(let char of str)
                console.log(char);